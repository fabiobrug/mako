# ===========================================
# LLM Provider Configuration
# ===========================================
# Supported providers: openai, anthropic, openrouter, gemini, deepseek, ollama
LLM_PROVIDER=openai
LLM_MODEL=gpt-4o-mini
LLM_API_KEY=sk-your-api-key-here

# For Ollama (local models)
# LLM_PROVIDER=ollama
# LLM_MODEL=llama3.2
# LLM_API_BASE=http://localhost:11434
#
# IMPORTANT: For Docker deployments with Ollama on host machine:
# Use host.docker.internal instead of localhost:
# LLM_API_BASE=http://host.docker.internal:11434
# (Works on macOS/Windows. On Linux, use your host IP or --network=host)

# For OpenRouter
# LLM_PROVIDER=openrouter
# LLM_MODEL=deepseek/deepseek-chat
# LLM_API_KEY=sk-or-v1-your-key

# For Anthropic
# LLM_PROVIDER=anthropic
# LLM_MODEL=claude-3-5-haiku-20241022
# LLM_API_KEY=sk-ant-your-key

# For Google Gemini
# LLM_PROVIDER=gemini
# LLM_MODEL=gemini-2.5-flash
# LLM_API_KEY=your-gemini-key

# For DeepSeek
# LLM_PROVIDER=deepseek
# LLM_MODEL=deepseek-chat
# LLM_API_KEY=your-deepseek-key

# ===========================================
# Embedding Provider Configuration
# ===========================================
# By default, uses the same provider as LLM
# You can override to use a different provider for embeddings
# (e.g., use local Ollama for embeddings, cloud for LLM)
# EMBEDDING_PROVIDER=ollama
# EMBEDDING_MODEL=nomic-embed-text
# EMBEDDING_API_KEY=
# EMBEDDING_API_BASE=http://localhost:11434
